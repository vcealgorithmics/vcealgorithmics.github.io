---
title: Understanding time complexity
---

You might have noticed by now that when creating an algorithm, there are many ways to skin a cat, and some of those ways are faster than others. Let's take a simple programming exercise like searching a list of sorted numbers

```python
list =  [7,3,1,4,3,2,1,4,6,8,9,3,2,1,5,67,3,2]
for item in list:

```

This is probably the first solution you thought of, but it ends up being quite inefficient. In the best case, we do pretty well, because it's the first element we check, but in the worst case, we have to go through the whole list. On average, you have to go through half the list!

 This might not seem so bad with a small list like this, but what if we wanted to go through millions or billions of values?  Imagine if you were in a library and you were looking for a book - you'd have to go through thousands of books before you found the one you wanted!

This is why we design and use faster algorithms. Let's take a look at a different way of searching, Binary Search.

```cancer


```

While not the fastest algorithm for searching out there, we can see instantly that blaHBLAHBLABHALBHALBHABLAHBL


We'll look at formal analysis in Unit 4, but for now a simple way of looking at how many times your algorithm does it's "basic operation"; that is, blah
balhba
blhalbhalh

balh